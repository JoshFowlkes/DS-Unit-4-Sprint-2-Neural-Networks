{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "A Neuron is a perceptron that takes a weighted sum of the inputs, adds bias, and passes through an activation function to determine if it passes information to the next stage.\n",
    "- **Input Layer:**\n",
    "The layer that takes in the input to a network/perceptron.\n",
    "- **Hidden Layer:**\n",
    "The layers that sits between the input and output layers, goes through forward feeding and backward propogation, and interacts with weights and biases.\n",
    "- **Output Layer:**\n",
    "The last layer that outputs the numbers for predictions such as classification, regression, etc.\n",
    "- **Activation:**\n",
    "An activation function transforms a matrix into a desirable shape for the output. The common activation functions are sigmoid, tanh, leaky ReLU, etc. And for regression problems the activation function is just a pass through, e.g., no activation function.\n",
    "- **Backpropagation:**\n",
    "Used to train neural networks, it is the calculating of gradients and updating of weights to be used in the neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate  bias\n",
       "0          0      1    1   1.0\n",
       "1          1      0    1   1.0\n",
       "2          0      1    1   1.0\n",
       "3          0      0    0   1.0\n",
       "4          1      1    0   1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 3), (10000,))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''  A funciton that takes 1 parameter, x, and returns the sigmoid calculation for it '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    ''' A function that takes 1 parameter, x, and returns the sigmoid derivative for it  '''\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    " \n",
    "\n",
    "def perceptron(inputs, outputs, num_passes):\n",
    "    ''' A Funcition that runs a basic neural network, the Perceptron. \n",
    "        It takes the inputs, the outputs to search for, as well as the number of passes\n",
    "        to learn off of. '''\n",
    "    \n",
    "    ''' Assigning Random weights for the inputs'''\n",
    "    weights = 2 * np.random.random((len(inputs.T),1)) \n",
    "    \n",
    "    for iteration in range(num_passes):\n",
    "        ''' Calculating the dot product of the inputs times the weights'''\n",
    "        weighted_sum = np.dot(inputs, weights) \n",
    "\n",
    "        ''' Output the activated value for the end of 1 Training epoch '''\n",
    "        activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "        ''' taking the difference of Output and True values and calculating the error '''\n",
    "        error = outputs - activated_output\n",
    "\n",
    "        ''' Where the magic happens... Gradiant Descent/Backdrop!! This is where the learning happens '''\n",
    "        adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "        ''' Updating the Weights after each iteration '''\n",
    "        weights += np.dot(inputs.T, adjustments)\n",
    "        \n",
    "        print('\\nEpoch ', iteration)\n",
    "        print('Weights after training:\\n', weights, '\\n')\n",
    "        print('Outputs After the Training:\\n',activated_output)\n",
    "    \n",
    "    return  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  0\n",
      "Weights after training:\n",
      " [[-315.85551186]\n",
      " [-308.56362431]\n",
      " [-612.96955606]] \n",
      "\n",
      "Outputs After the Training:\n",
      " [[0.77922382]\n",
      " [0.79241912]\n",
      " [0.77922382]\n",
      " ...\n",
      " [0.77922382]\n",
      " [0.77922382]\n",
      " [0.79241912]]\n",
      "\n",
      "Epoch  1\n",
      "Weights after training:\n",
      " [[308.89448814]\n",
      " [316.43637569]\n",
      " [637.03044394]] \n",
      "\n",
      "Outputs After the Training:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Epoch  2\n",
      "Weights after training:\n",
      " [[-181.0624495 ]\n",
      " [-173.71717388]\n",
      " [-346.02922227]] \n",
      "\n",
      "Outputs After the Training:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "perceptron(X, y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        '''Set up Architecture of Neural Network'''\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 10000\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        \n",
    "        ''' Initial Weights '''\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes) + 1\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) + 1\n",
    "   \n",
    "\n",
    "    ''' Sigmoid Function '''    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    \n",
    "    ''' Sigmoid Derivative Function '''\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s) \n",
    "    \n",
    "    \n",
    "    ''' Feed Forward Function '''\n",
    "    def feed_forward(self, X):\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "    \n",
    "    \n",
    "    ''' Backward Propogation through the network function '''   \n",
    "    def backward(self, X,y,o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "    ''' Defining the Training Function '''\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "''' For Loop Epoch Printing from Lecture 2 '''\n",
    "for i in range(5):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Imports and such  '''\n",
    "#!pip install keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "20    59    1   0       135   234    0        1      161      0      0.5   \n",
       "242   64    1   0       145   212    0        0      132      0      2.0   \n",
       "5     57    1   0       140   192    0        1      148      0      0.4   \n",
       "43    53    0   0       130   264    0        0      143      0      0.4   \n",
       "288   57    1   0       110   335    0        1      143      1      3.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "20       1   0     3       1  \n",
       "242      1   2     1       0  \n",
       "5        1   0     1       1  \n",
       "43       1   0     2       1  \n",
       "288      1   1     3       0  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Assigning x and y TRAIN BABAY '''\n",
    "X_train = df.loc[:, df.columns != 'target'].values\n",
    "y_train = df.loc[:, df.columns == 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[59.,  1.,  0., ...,  1.,  0.,  3.],\n",
       "       [64.,  1.,  0., ...,  1.,  2.,  1.],\n",
       "       [57.,  1.,  0., ...,  1.,  0.,  1.],\n",
       "       ...,\n",
       "       [52.,  1.,  1., ...,  2.,  0.,  2.],\n",
       "       [74.,  0.,  1., ...,  2.,  1.,  2.],\n",
       "       [54.,  0.,  2., ...,  2.,  0.,  2.]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Pre normalization '''\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Normalizing the data  '''\n",
    "#X_train = keras.utils.normalize(X_train, axis=1, order=2)\n",
    "#y_train = keras.utils.normalize(y_train, axis=1, order=2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5110413 ,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "        -0.71442887,  1.12302895],\n",
       "       [ 1.06248543,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "         1.24459328, -2.14887271],\n",
       "       [ 0.29046364,  0.68100522, -0.93851463, ..., -0.64911323,\n",
       "        -0.71442887, -2.14887271],\n",
       "       ...,\n",
       "       [-0.26098049,  0.68100522,  0.03203122, ...,  0.97635214,\n",
       "        -0.71442887, -0.51292188],\n",
       "       [ 2.16537369, -1.46841752,  0.03203122, ...,  0.97635214,\n",
       "         0.26508221, -0.51292188],\n",
       "       [-0.04040284, -1.46841752,  1.00257707, ...,  0.97635214,\n",
       "        -0.71442887, -0.51292188]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Checking for normalization '''\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "303/303 [==============================] - 12s 38ms/step - loss: 1610263.1668 - acc: 0.4719\n",
      "Epoch 2/15\n",
      "303/303 [==============================] - 0s 450us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 3/15\n",
      "303/303 [==============================] - 0s 549us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 4/15\n",
      "303/303 [==============================] - 0s 513us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 5/15\n",
      "303/303 [==============================] - 0s 559us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 6/15\n",
      "303/303 [==============================] - 0s 522us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 7/15\n",
      "303/303 [==============================] - 0s 482us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 8/15\n",
      "303/303 [==============================] - 0s 600us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 9/15\n",
      "303/303 [==============================] - 0s 588us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 10/15\n",
      "303/303 [==============================] - 0s 481us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 11/15\n",
      "303/303 [==============================] - 0s 492us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 12/15\n",
      "303/303 [==============================] - 0s 457us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 13/15\n",
      "303/303 [==============================] - 0s 499us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 14/15\n",
      "303/303 [==============================] - 0s 496us/step - loss: 54.4554 - acc: 0.4554\n",
      "Epoch 15/15\n",
      "303/303 [==============================] - 0s 432us/step - loss: 54.4554 - acc: 0.4554\n"
     ]
    }
   ],
   "source": [
    "''' Baseline Model '''\n",
    "input_dim = (len(df.columns)-1)\n",
    "epochs=15\n",
    "batch_size=10\n",
    "\n",
    "''' Making the sexy model '''\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(input_dim*2, input_dim=input_dim, activation='relu'))\n",
    "model_1.add(Dense(1, activation='sigmoid')) # Using sigmoid since its binary classification and can converge faster\n",
    "model_1.compile(loss = 'mean_absolute_percentage_error',\n",
    "              optimizer = 'sgd',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "''' Fitting the model '''\n",
    "fitting = model_1.fit(X_train, y_train,\n",
    "                      epochs = epochs,\n",
    "                      batch_size = batch_size,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' So it appears that having the sigmoid in the activation function of the final layer is \n",
    "    getting the accuracy stuck at one value for each epoch, will fix this in the fine tuning model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8217821786112518 using {'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "''' Hyper Parameter Tuned Model '''\n",
    "\n",
    "input_dim = (len(df.columns) - 1)\n",
    "epochs = 10\n",
    "batch_size=10\n",
    "momentum= 0.01\n",
    "\n",
    "''' Building the Model '''\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_dim**2, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(input_dim*3, activation='sigmoid'))\n",
    "    model.add(Dense(input_dim*2, activation='sigmoid'))\n",
    "    \n",
    "    model.add(Dense(1,)) #activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss = 'mse',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics = ['accuracy'],)\n",
    "    return model\n",
    "\n",
    "\n",
    "''' Grid Search CV '''\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "''' The fine tuning parameters, minimum 3 for a 3 '''\n",
    "param_grid = {#'batch_size': [10, 20, 30],\n",
    "              'epochs': [10,20],\n",
    "              #'shuffle' : ['True','False'],\n",
    "              #'validation_split' : [0.2, 0.3 ,0.4],\n",
    "\n",
    "             }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' I commented out stuff on the param grid as I went because if i did too much at once it took \n",
    "    wayyyyy too long to finish computations\n",
    "'''\n",
    "\n",
    "''' ALso the optimizers I tried:  '''\n",
    "# with 10 epochs\n",
    "#adam 82\n",
    "#rmsprop 81\n",
    "#Nadam 82\n",
    "#sgd 54\n",
    "#adamax 84\n",
    "#adagrad 81.5\n",
    "#adadelta 84.15\n",
    "\n",
    "''' loss function results '''\n",
    "#mse 84.15\n",
    "#mae 81.8\n",
    "#binary_crossentropy 22.7 hahahah "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
