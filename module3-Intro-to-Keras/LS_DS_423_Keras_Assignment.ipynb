{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Imports '''\n",
    "#!pip install tensorflow==1.14.0 keras==2.2.4\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend\n",
    "\n",
    "''' Importing the DataSet '''\n",
    "from keras.datasets import boston_housing\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Building the Model '''\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "model.add(BatchNormalization()) # normalizing the data \n",
    "model.add(Dense(13, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "''' Compiling the model '''\n",
    "model.compile(loss='mean_absolute_percentage_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "404/404 [==============================] - 1s 3ms/step - loss: 99.2510 - acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 98.2096 - acc: 0.0000e+00\n",
      "Epoch 3/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 96.7795 - acc: 0.0000e+00\n",
      "Epoch 4/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 94.9901 - acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 92.8047 - acc: 0.0000e+00\n",
      "Epoch 6/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 90.2964 - acc: 0.0000e+00\n",
      "Epoch 7/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 87.6135 - acc: 0.0025\n",
      "Epoch 8/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 84.8956 - acc: 0.0000e+00\n",
      "Epoch 9/150\n",
      "404/404 [==============================] - 0s 71us/step - loss: 81.6961 - acc: 0.0000e+00\n",
      "Epoch 10/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 78.5864 - acc: 0.0000e+00\n",
      "Epoch 11/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 75.1937 - acc: 0.0025\n",
      "Epoch 12/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 72.2635 - acc: 0.0000e+00\n",
      "Epoch 13/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 68.8605 - acc: 0.0025\n",
      "Epoch 14/150\n",
      "404/404 [==============================] - 0s 79us/step - loss: 65.5556 - acc: 0.0000e+00\n",
      "Epoch 15/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 61.9400 - acc: 0.0000e+00\n",
      "Epoch 16/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 58.8433 - acc: 0.0000e+00\n",
      "Epoch 17/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 55.4600 - acc: 0.0000e+00\n",
      "Epoch 18/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 51.6324 - acc: 0.0000e+00\n",
      "Epoch 19/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 47.9240 - acc: 0.0000e+00\n",
      "Epoch 20/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 43.4975 - acc: 0.0025\n",
      "Epoch 21/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 39.1046 - acc: 0.0025\n",
      "Epoch 22/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 34.7185 - acc: 0.0025\n",
      "Epoch 23/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 31.1508 - acc: 0.0050\n",
      "Epoch 24/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 28.5071 - acc: 0.0025\n",
      "Epoch 25/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 26.6217 - acc: 0.0074\n",
      "Epoch 26/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 25.8912 - acc: 0.0124\n",
      "Epoch 27/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 24.5984 - acc: 0.0050\n",
      "Epoch 28/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 24.2688 - acc: 0.0050\n",
      "Epoch 29/150\n",
      "404/404 [==============================] - 0s 71us/step - loss: 23.8149 - acc: 0.0099\n",
      "Epoch 30/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 23.4684 - acc: 0.0173\n",
      "Epoch 31/150\n",
      "404/404 [==============================] - 0s 76us/step - loss: 22.9593 - acc: 0.0149\n",
      "Epoch 32/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 22.5465 - acc: 0.0074\n",
      "Epoch 33/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 22.3621 - acc: 0.0099\n",
      "Epoch 34/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 20.7362 - acc: 0.0149\n",
      "Epoch 35/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 20.4491 - acc: 0.0173\n",
      "Epoch 36/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 20.0753 - acc: 0.0149\n",
      "Epoch 37/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 19.9327 - acc: 0.0124\n",
      "Epoch 38/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 19.5651 - acc: 0.0124\n",
      "Epoch 39/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 19.5890 - acc: 0.0074\n",
      "Epoch 40/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 19.2284 - acc: 0.0074\n",
      "Epoch 41/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 18.8371 - acc: 0.0223\n",
      "Epoch 42/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 18.8595 - acc: 0.0173\n",
      "Epoch 43/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 19.0082 - acc: 0.0124\n",
      "Epoch 44/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 18.7098 - acc: 0.0099\n",
      "Epoch 45/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 18.4837 - acc: 0.0124\n",
      "Epoch 46/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 19.1047 - acc: 0.0223\n",
      "Epoch 47/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 18.4493 - acc: 0.0198\n",
      "Epoch 48/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 18.8682 - acc: 0.0099\n",
      "Epoch 49/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 18.5118 - acc: 0.0198\n",
      "Epoch 50/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 17.5380 - acc: 0.0124\n",
      "Epoch 51/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 17.3523 - acc: 0.0272\n",
      "Epoch 52/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 18.2745 - acc: 0.0074\n",
      "Epoch 53/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 17.5886 - acc: 0.0198\n",
      "Epoch 54/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 17.1786 - acc: 0.0124\n",
      "Epoch 55/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 17.0916 - acc: 0.0074\n",
      "Epoch 56/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 17.1523 - acc: 0.0099\n",
      "Epoch 57/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 17.5601 - acc: 0.0173\n",
      "Epoch 58/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 17.2017 - acc: 0.0173\n",
      "Epoch 59/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 17.1063 - acc: 0.0074\n",
      "Epoch 60/150\n",
      "404/404 [==============================] - 0s 54us/step - loss: 17.2854 - acc: 0.0099\n",
      "Epoch 61/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 16.9050 - acc: 0.0025\n",
      "Epoch 62/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 16.7579 - acc: 0.0099\n",
      "Epoch 63/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 16.6011 - acc: 0.0149\n",
      "Epoch 64/150\n",
      "404/404 [==============================] - 0s 52us/step - loss: 16.7731 - acc: 0.0099\n",
      "Epoch 65/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 16.6101 - acc: 0.0124\n",
      "Epoch 66/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 17.1662 - acc: 0.0074\n",
      "Epoch 67/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 16.8945 - acc: 0.0198\n",
      "Epoch 68/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 16.2851 - acc: 0.0050\n",
      "Epoch 69/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 16.2529 - acc: 0.0149\n",
      "Epoch 70/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 16.0529 - acc: 0.0124\n",
      "Epoch 71/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 16.5736 - acc: 0.0124\n",
      "Epoch 72/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 16.6946 - acc: 0.0099\n",
      "Epoch 73/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 16.3401 - acc: 0.0124\n",
      "Epoch 74/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 16.4925 - acc: 0.0099\n",
      "Epoch 75/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 16.6617 - acc: 0.0173\n",
      "Epoch 76/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 16.1013 - acc: 0.0149\n",
      "Epoch 77/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 16.2550 - acc: 0.0000e+00\n",
      "Epoch 78/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 17.5256 - acc: 0.0099\n",
      "Epoch 79/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 16.5412 - acc: 0.0074\n",
      "Epoch 80/150\n",
      "404/404 [==============================] - 0s 54us/step - loss: 17.0184 - acc: 0.0099\n",
      "Epoch 81/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 16.8628 - acc: 0.0099\n",
      "Epoch 82/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 16.1043 - acc: 0.0149\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 0s 58us/step - loss: 16.2928 - acc: 0.0124\n",
      "Epoch 84/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 16.7143 - acc: 0.0025\n",
      "Epoch 85/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 16.1721 - acc: 0.0124\n",
      "Epoch 86/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 16.2404 - acc: 0.0173\n",
      "Epoch 87/150\n",
      "404/404 [==============================] - 0s 52us/step - loss: 16.6612 - acc: 0.0099\n",
      "Epoch 88/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 15.8853 - acc: 0.0198\n",
      "Epoch 89/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 16.2696 - acc: 0.0074\n",
      "Epoch 90/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 16.1882 - acc: 0.0099\n",
      "Epoch 91/150\n",
      "404/404 [==============================] - 0s 50us/step - loss: 16.1625 - acc: 0.0124\n",
      "Epoch 92/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 16.2647 - acc: 0.0124\n",
      "Epoch 93/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 16.0107 - acc: 0.0149\n",
      "Epoch 94/150\n",
      "404/404 [==============================] - 0s 53us/step - loss: 15.8036 - acc: 0.0198\n",
      "Epoch 95/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 16.4021 - acc: 0.0149\n",
      "Epoch 96/150\n",
      "404/404 [==============================] - 0s 54us/step - loss: 16.2778 - acc: 0.0124\n",
      "Epoch 97/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 16.5889 - acc: 0.0124\n",
      "Epoch 98/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 16.0366 - acc: 0.0099\n",
      "Epoch 99/150\n",
      "404/404 [==============================] - 0s 54us/step - loss: 16.3548 - acc: 0.0099\n",
      "Epoch 100/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 16.1909 - acc: 0.0074\n",
      "Epoch 101/150\n",
      "404/404 [==============================] - 0s 53us/step - loss: 16.0721 - acc: 0.0099\n",
      "Epoch 102/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 15.5168 - acc: 0.0173\n",
      "Epoch 103/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 15.8733 - acc: 0.0074\n",
      "Epoch 104/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 15.0097 - acc: 0.0124\n",
      "Epoch 105/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 15.9301 - acc: 0.0074\n",
      "Epoch 106/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 15.7486 - acc: 0.0173\n",
      "Epoch 107/150\n",
      "404/404 [==============================] - 0s 53us/step - loss: 16.6757 - acc: 0.0124\n",
      "Epoch 108/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 16.0807 - acc: 0.0124\n",
      "Epoch 109/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 15.7841 - acc: 0.0173\n",
      "Epoch 110/150\n",
      "404/404 [==============================] - 0s 54us/step - loss: 16.1407 - acc: 0.0124\n",
      "Epoch 111/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 16.1072 - acc: 0.0074\n",
      "Epoch 112/150\n",
      "404/404 [==============================] - 0s 52us/step - loss: 15.9977 - acc: 0.0223\n",
      "Epoch 113/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 15.5543 - acc: 0.0124\n",
      "Epoch 114/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 15.0836 - acc: 0.0149\n",
      "Epoch 115/150\n",
      "404/404 [==============================] - 0s 51us/step - loss: 15.7522 - acc: 0.0149\n",
      "Epoch 116/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 16.2289 - acc: 0.0000e+00\n",
      "Epoch 117/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 15.3648 - acc: 0.0099\n",
      "Epoch 118/150\n",
      "404/404 [==============================] - 0s 50us/step - loss: 15.3293 - acc: 0.0149\n",
      "Epoch 119/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 15.8267 - acc: 0.0173\n",
      "Epoch 120/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 15.3178 - acc: 0.0223\n",
      "Epoch 121/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 15.7578 - acc: 0.0050\n",
      "Epoch 122/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 15.4977 - acc: 0.0124\n",
      "Epoch 123/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 15.8128 - acc: 0.0074\n",
      "Epoch 124/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 15.4140 - acc: 0.0173\n",
      "Epoch 125/150\n",
      "404/404 [==============================] - 0s 52us/step - loss: 15.0019 - acc: 0.0124\n",
      "Epoch 126/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 16.0026 - acc: 0.0124\n",
      "Epoch 127/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 15.7176 - acc: 0.0099\n",
      "Epoch 128/150\n",
      "404/404 [==============================] - 0s 52us/step - loss: 15.8514 - acc: 0.0124\n",
      "Epoch 129/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 15.9134 - acc: 0.0149\n",
      "Epoch 130/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 15.0977 - acc: 0.0074\n",
      "Epoch 131/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 15.7476 - acc: 0.0099\n",
      "Epoch 132/150\n",
      "404/404 [==============================] - 0s 51us/step - loss: 15.0607 - acc: 0.0074\n",
      "Epoch 133/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 15.9686 - acc: 0.0074\n",
      "Epoch 134/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 15.8350 - acc: 0.0124\n",
      "Epoch 135/150\n",
      "404/404 [==============================] - 0s 50us/step - loss: 16.1035 - acc: 0.0025\n",
      "Epoch 136/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 15.0901 - acc: 0.0124\n",
      "Epoch 137/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 15.6011 - acc: 0.0149\n",
      "Epoch 138/150\n",
      "404/404 [==============================] - 0s 53us/step - loss: 15.1932 - acc: 0.0149\n",
      "Epoch 139/150\n",
      "404/404 [==============================] - 0s 55us/step - loss: 16.4499 - acc: 0.0149\n",
      "Epoch 140/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 15.3336 - acc: 0.0149\n",
      "Epoch 141/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 15.7567 - acc: 0.0074\n",
      "Epoch 142/150\n",
      "404/404 [==============================] - 0s 53us/step - loss: 15.5506 - acc: 0.0173\n",
      "Epoch 143/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 15.4257 - acc: 0.0149\n",
      "Epoch 144/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 15.0151 - acc: 0.0099\n",
      "Epoch 145/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 15.5376 - acc: 0.0173\n",
      "Epoch 146/150\n",
      "404/404 [==============================] - 0s 53us/step - loss: 15.3736 - acc: 0.0000e+00\n",
      "Epoch 147/150\n",
      "404/404 [==============================] - 0s 52us/step - loss: 15.5316 - acc: 0.0173\n",
      "Epoch 148/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 15.0584 - acc: 0.0099\n",
      "Epoch 149/150\n",
      "404/404 [==============================] - 0s 52us/step - loss: 14.9248 - acc: 0.0198\n",
      "Epoch 150/150\n",
      "404/404 [==============================] - 0s 54us/step - loss: 15.2844 - acc: 0.0099\n",
      "102/102 [==============================] - 0s 4ms/step\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "''' Fitting the model '''\n",
    "fitting = model.fit(x_train, y_train, epochs=150)\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print((scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square error 23.195599256422938 Mean Absolute Error 3.4641858124067126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = linear_model.predict(x_test)\n",
    "\n",
    "print('Mean Square error', mean_squared_error(y_test, y_pred),\n",
    "      'Mean Absolute Error', mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# ''' Niceeee so 150 epochs of a total vanilla neural network did better than linear regression '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Now to better optimize the neural net and try to get a very very low mean aboslute error \\n    Along with some feature engineering\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Now to better optimize the neural net and try to get a very very low mean aboslute error \n",
    "    Along with some feature engineering\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(169, input_dim=13, activation='relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Dense(13, activation='sigmoid'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_2.compile(loss='mean_absolute_percentage_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 15.1227 - acc: 0.0223\n",
      "Epoch 2/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 14.7439 - acc: 0.0124\n",
      "Epoch 3/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 15.0046 - acc: 0.0074\n",
      "Epoch 4/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.7474 - acc: 0.0124\n",
      "Epoch 5/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 15.3418 - acc: 0.0149\n",
      "Epoch 6/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 15.6506 - acc: 0.0025\n",
      "Epoch 7/150\n",
      "404/404 [==============================] - 0s 75us/step - loss: 15.1647 - acc: 0.0099\n",
      "Epoch 8/150\n",
      "404/404 [==============================] - 0s 81us/step - loss: 14.7330 - acc: 0.0149\n",
      "Epoch 9/150\n",
      "404/404 [==============================] - 0s 79us/step - loss: 14.7179 - acc: 0.0099\n",
      "Epoch 10/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 15.1063 - acc: 0.0025\n",
      "Epoch 11/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 15.0985 - acc: 0.0124\n",
      "Epoch 12/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 15.3008 - acc: 0.0050\n",
      "Epoch 13/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.8341 - acc: 0.0025\n",
      "Epoch 14/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.8629 - acc: 0.0173\n",
      "Epoch 15/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 15.1482 - acc: 0.0198\n",
      "Epoch 16/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.9038 - acc: 0.0149\n",
      "Epoch 17/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.9269 - acc: 0.0124\n",
      "Epoch 18/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.6919 - acc: 0.0149\n",
      "Epoch 19/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 15.7373 - acc: 0.0124\n",
      "Epoch 20/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.9893 - acc: 0.0198\n",
      "Epoch 21/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 14.6412 - acc: 0.0099\n",
      "Epoch 22/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 15.9865 - acc: 0.0099\n",
      "Epoch 23/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 15.0777 - acc: 0.0124\n",
      "Epoch 24/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.3556 - acc: 0.0099\n",
      "Epoch 25/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 15.2334 - acc: 0.0050\n",
      "Epoch 26/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 15.3817 - acc: 0.0124\n",
      "Epoch 27/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.8214 - acc: 0.0050\n",
      "Epoch 28/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 14.9135 - acc: 0.0223\n",
      "Epoch 29/150\n",
      "404/404 [==============================] - 0s 71us/step - loss: 15.0692 - acc: 0.0124\n",
      "Epoch 30/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 15.1586 - acc: 0.0124\n",
      "Epoch 31/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.9180 - acc: 0.0099\n",
      "Epoch 32/150\n",
      "404/404 [==============================] - 0s 73us/step - loss: 14.6928 - acc: 0.0149\n",
      "Epoch 33/150\n",
      "404/404 [==============================] - 0s 78us/step - loss: 14.7164 - acc: 0.0173\n",
      "Epoch 34/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 14.8219 - acc: 0.0198\n",
      "Epoch 35/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.7001 - acc: 0.0198\n",
      "Epoch 36/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.4140 - acc: 0.0074\n",
      "Epoch 37/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 15.0390 - acc: 0.0074\n",
      "Epoch 38/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.9774 - acc: 0.0099\n",
      "Epoch 39/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 14.3571 - acc: 0.0099\n",
      "Epoch 40/150\n",
      "404/404 [==============================] - 0s 71us/step - loss: 14.4355 - acc: 0.0173\n",
      "Epoch 41/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 15.5291 - acc: 0.0124\n",
      "Epoch 42/150\n",
      "404/404 [==============================] - 0s 73us/step - loss: 15.0260 - acc: 0.0074\n",
      "Epoch 43/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.4247 - acc: 0.0099\n",
      "Epoch 44/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.3948 - acc: 0.0050\n",
      "Epoch 45/150\n",
      "404/404 [==============================] - 0s 74us/step - loss: 14.3409 - acc: 0.0124\n",
      "Epoch 46/150\n",
      "404/404 [==============================] - 0s 74us/step - loss: 14.4980 - acc: 0.0149\n",
      "Epoch 47/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 14.4364 - acc: 0.0099\n",
      "Epoch 48/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 14.6319 - acc: 0.0149\n",
      "Epoch 49/150\n",
      "404/404 [==============================] - 0s 75us/step - loss: 14.4157 - acc: 0.0149\n",
      "Epoch 50/150\n",
      "404/404 [==============================] - 0s 73us/step - loss: 14.0744 - acc: 0.0099\n",
      "Epoch 51/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 15.0255 - acc: 0.0149\n",
      "Epoch 52/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 14.9304 - acc: 0.0050\n",
      "Epoch 53/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.8707 - acc: 0.0124\n",
      "Epoch 54/150\n",
      "404/404 [==============================] - 0s 77us/step - loss: 14.2766 - acc: 0.0198\n",
      "Epoch 55/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 14.8511 - acc: 0.0173\n",
      "Epoch 56/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 15.3456 - acc: 0.0099\n",
      "Epoch 57/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 14.3147 - acc: 0.0099\n",
      "Epoch 58/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.9877 - acc: 0.0124\n",
      "Epoch 59/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 14.8503 - acc: 0.0099\n",
      "Epoch 60/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.2271 - acc: 0.0074\n",
      "Epoch 61/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.4948 - acc: 0.0198\n",
      "Epoch 62/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 15.1045 - acc: 0.0149\n",
      "Epoch 63/150\n",
      "404/404 [==============================] - 0s 70us/step - loss: 14.4971 - acc: 0.0173\n",
      "Epoch 64/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 14.3744 - acc: 0.0124\n",
      "Epoch 65/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 14.4581 - acc: 0.0050\n",
      "Epoch 66/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 14.7399 - acc: 0.0099\n",
      "Epoch 67/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.5335 - acc: 0.0173\n",
      "Epoch 68/150\n",
      "404/404 [==============================] - 0s 73us/step - loss: 14.5969 - acc: 0.0099\n",
      "Epoch 69/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 15.1356 - acc: 0.0124\n",
      "Epoch 70/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 15.3619 - acc: 0.0074\n",
      "Epoch 71/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.1260 - acc: 0.0124\n",
      "Epoch 72/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.5205 - acc: 0.0149\n",
      "Epoch 73/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 14.2170 - acc: 0.0099\n",
      "Epoch 74/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 14.9315 - acc: 0.0050\n",
      "Epoch 75/150\n",
      "404/404 [==============================] - 0s 73us/step - loss: 14.6132 - acc: 0.0198\n",
      "Epoch 76/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.1144 - acc: 0.0099\n",
      "Epoch 77/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.6837 - acc: 0.0223\n",
      "Epoch 78/150\n",
      "404/404 [==============================] - 0s 67us/step - loss: 14.6095 - acc: 0.0050\n",
      "Epoch 79/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.6439 - acc: 0.0050\n",
      "Epoch 80/150\n",
      "404/404 [==============================] - 0s 74us/step - loss: 14.4215 - acc: 0.0223\n",
      "Epoch 81/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.8960 - acc: 0.0149\n",
      "Epoch 82/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.7728 - acc: 0.0074\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 0s 65us/step - loss: 14.6794 - acc: 0.0099\n",
      "Epoch 84/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.2316 - acc: 0.0198\n",
      "Epoch 85/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 13.9103 - acc: 0.0124\n",
      "Epoch 86/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 14.3297 - acc: 0.0074\n",
      "Epoch 87/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.8176 - acc: 0.0124\n",
      "Epoch 88/150\n",
      "404/404 [==============================] - 0s 54us/step - loss: 14.3539 - acc: 0.0198\n",
      "Epoch 89/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 14.2930 - acc: 0.0149\n",
      "Epoch 90/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 13.7132 - acc: 0.0099\n",
      "Epoch 91/150\n",
      "404/404 [==============================] - 0s 86us/step - loss: 14.3090 - acc: 0.0124\n",
      "Epoch 92/150\n",
      "404/404 [==============================] - 0s 75us/step - loss: 14.5626 - acc: 0.0099\n",
      "Epoch 93/150\n",
      "404/404 [==============================] - 0s 73us/step - loss: 14.0940 - acc: 0.0124\n",
      "Epoch 94/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 14.0514 - acc: 0.0124\n",
      "Epoch 95/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 14.1221 - acc: 0.0223\n",
      "Epoch 96/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 15.2297 - acc: 0.0099\n",
      "Epoch 97/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 14.4943 - acc: 0.0149\n",
      "Epoch 98/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 13.7448 - acc: 0.0149\n",
      "Epoch 99/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 13.6534 - acc: 0.0074\n",
      "Epoch 100/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 14.3408 - acc: 0.0099\n",
      "Epoch 101/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 14.0948 - acc: 0.0124\n",
      "Epoch 102/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 14.1083 - acc: 0.0173\n",
      "Epoch 103/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 14.7268 - acc: 0.0124\n",
      "Epoch 104/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 14.1742 - acc: 0.0173\n",
      "Epoch 105/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 14.5739 - acc: 0.0149\n",
      "Epoch 106/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 14.7031 - acc: 0.0149\n",
      "Epoch 107/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 14.1593 - acc: 0.0124\n",
      "Epoch 108/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.3861 - acc: 0.0173\n",
      "Epoch 109/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 14.3207 - acc: 0.0074\n",
      "Epoch 110/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 13.7881 - acc: 0.0124\n",
      "Epoch 111/150\n",
      "404/404 [==============================] - 0s 58us/step - loss: 14.6904 - acc: 0.0099\n",
      "Epoch 112/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 14.0672 - acc: 0.0124\n",
      "Epoch 113/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 14.2569 - acc: 0.0173\n",
      "Epoch 114/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 14.2530 - acc: 0.0050\n",
      "Epoch 115/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.1886 - acc: 0.0124\n",
      "Epoch 116/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 14.5668 - acc: 0.0173\n",
      "Epoch 117/150\n",
      "404/404 [==============================] - 0s 57us/step - loss: 14.1346 - acc: 0.0050\n",
      "Epoch 118/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 14.1227 - acc: 0.0149\n",
      "Epoch 119/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 13.6134 - acc: 0.0173\n",
      "Epoch 120/150\n",
      "404/404 [==============================] - 0s 59us/step - loss: 14.3463 - acc: 0.0050\n",
      "Epoch 121/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 13.7417 - acc: 0.0099\n",
      "Epoch 122/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.0384 - acc: 0.0074\n",
      "Epoch 123/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 13.9556 - acc: 0.0149\n",
      "Epoch 124/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 14.1096 - acc: 0.0173\n",
      "Epoch 125/150\n",
      "404/404 [==============================] - 0s 61us/step - loss: 13.4252 - acc: 0.0099\n",
      "Epoch 126/150\n",
      "404/404 [==============================] - 0s 64us/step - loss: 13.5801 - acc: 0.0099\n",
      "Epoch 127/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.3777 - acc: 0.0149\n",
      "Epoch 128/150\n",
      "404/404 [==============================] - 0s 77us/step - loss: 13.7619 - acc: 0.0198\n",
      "Epoch 129/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 13.7941 - acc: 0.0124\n",
      "Epoch 130/150\n",
      "404/404 [==============================] - 0s 56us/step - loss: 13.7671 - acc: 0.0198\n",
      "Epoch 131/150\n",
      "404/404 [==============================] - 0s 90us/step - loss: 14.0900 - acc: 0.0149\n",
      "Epoch 132/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 14.0629 - acc: 0.0173\n",
      "Epoch 133/150\n",
      "404/404 [==============================] - 0s 74us/step - loss: 13.8243 - acc: 0.0124\n",
      "Epoch 134/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 14.0317 - acc: 0.0099\n",
      "Epoch 135/150\n",
      "404/404 [==============================] - 0s 66us/step - loss: 13.8066 - acc: 0.0198\n",
      "Epoch 136/150\n",
      "404/404 [==============================] - 0s 63us/step - loss: 14.1730 - acc: 0.0074\n",
      "Epoch 137/150\n",
      "404/404 [==============================] - 0s 71us/step - loss: 13.8989 - acc: 0.0099\n",
      "Epoch 138/150\n",
      "404/404 [==============================] - 0s 62us/step - loss: 13.9564 - acc: 0.0025\n",
      "Epoch 139/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 14.2604 - acc: 0.0198\n",
      "Epoch 140/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 14.6072 - acc: 0.0149\n",
      "Epoch 141/150\n",
      "404/404 [==============================] - 0s 65us/step - loss: 13.8659 - acc: 0.0198\n",
      "Epoch 142/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 14.2555 - acc: 0.0124\n",
      "Epoch 143/150\n",
      "404/404 [==============================] - 0s 68us/step - loss: 14.6460 - acc: 0.0173\n",
      "Epoch 144/150\n",
      "404/404 [==============================] - 0s 73us/step - loss: 13.6071 - acc: 0.0099\n",
      "Epoch 145/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 13.4239 - acc: 0.0198\n",
      "Epoch 146/150\n",
      "404/404 [==============================] - 0s 72us/step - loss: 13.3270 - acc: 0.0074\n",
      "Epoch 147/150\n",
      "404/404 [==============================] - 0s 83us/step - loss: 13.3353 - acc: 0.0149\n",
      "Epoch 148/150\n",
      "404/404 [==============================] - 0s 69us/step - loss: 13.9314 - acc: 0.0099\n",
      "Epoch 149/150\n",
      "404/404 [==============================] - 0s 60us/step - loss: 13.8046 - acc: 0.0149\n",
      "Epoch 150/150\n",
      "404/404 [==============================] - 0s 75us/step - loss: 13.1947 - acc: 0.0124\n",
      "102/102 [==============================] - 0s 59us/step\n"
     ]
    }
   ],
   "source": [
    "fitting = model.fit(x_train, y_train, epochs=150)\n",
    "scores = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist \n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
       "          1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
       "          0,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
       "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
       "         10,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
       "         72,  15],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
       "        172,  66],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
       "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
       "        229,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
       "        173,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
       "        202,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
       "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
       "        209,  52],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
       "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
       "        167,  56],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "         92,   0],\n",
       "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
       "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
       "         77,   0],\n",
       "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
       "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
       "        159,   0],\n",
       "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
       "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
       "        215,   0],\n",
       "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
       "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
       "        246,   0],\n",
       "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
       "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
       "        225,   0],\n",
       "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
       "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
       "        229,  29],\n",
       "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
       "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
       "        230,  67],\n",
       "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
       "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
       "        206, 115],\n",
       "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
       "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
       "        210,  92],\n",
       "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
       "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
       "        170,   0],\n",
       "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
       "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "''' Preprocessing the data '''\n",
    "\n",
    "X_train = x_train.reshape(60000, 784)\n",
    "X_test = x_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00392157, 0.        , 0.        , 0.05098039,\n",
       "       0.28627452, 0.        , 0.        , 0.00392157, 0.01568628,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "       0.        , 0.14117648, 0.53333336, 0.49803922, 0.24313726,\n",
       "       0.21176471, 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.01176471, 0.01568628, 0.        , 0.        , 0.01176471,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
       "       0.8       , 0.6901961 , 0.5254902 , 0.5647059 , 0.48235294,\n",
       "       0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04705882, 0.03921569, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.60784316, 0.9254902 , 0.8117647 ,\n",
       "       0.69803923, 0.41960785, 0.6117647 , 0.6313726 , 0.42745098,\n",
       "       0.2509804 , 0.09019608, 0.3019608 , 0.50980395, 0.28235295,\n",
       "       0.05882353, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.        , 0.27058825,\n",
       "       0.8117647 , 0.8745098 , 0.85490197, 0.84705883, 0.84705883,\n",
       "       0.6392157 , 0.49803922, 0.4745098 , 0.47843137, 0.57254905,\n",
       "       0.5529412 , 0.34509805, 0.6745098 , 0.25882354, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00392157, 0.00392157,\n",
       "       0.00392157, 0.        , 0.78431374, 0.9098039 , 0.9098039 ,\n",
       "       0.9137255 , 0.8980392 , 0.8745098 , 0.8745098 , 0.84313726,\n",
       "       0.8352941 , 0.6431373 , 0.49803922, 0.48235294, 0.76862746,\n",
       "       0.8980392 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.7176471 , 0.88235295, 0.84705883, 0.8745098 , 0.89411765,\n",
       "       0.92156863, 0.8901961 , 0.8784314 , 0.87058824, 0.8784314 ,\n",
       "       0.8666667 , 0.8745098 , 0.9607843 , 0.6784314 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.75686276, 0.89411765,\n",
       "       0.85490197, 0.8352941 , 0.7764706 , 0.7058824 , 0.83137256,\n",
       "       0.8235294 , 0.827451  , 0.8352941 , 0.8745098 , 0.8627451 ,\n",
       "       0.9529412 , 0.7921569 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.01176471, 0.        ,\n",
       "       0.04705882, 0.85882354, 0.8627451 , 0.83137256, 0.85490197,\n",
       "       0.7529412 , 0.6627451 , 0.8901961 , 0.8156863 , 0.85490197,\n",
       "       0.8784314 , 0.83137256, 0.8862745 , 0.77254903, 0.81960785,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.02352941, 0.        , 0.3882353 , 0.95686275,\n",
       "       0.87058824, 0.8627451 , 0.85490197, 0.79607844, 0.7764706 ,\n",
       "       0.8666667 , 0.84313726, 0.8352941 , 0.87058824, 0.8627451 ,\n",
       "       0.9607843 , 0.46666667, 0.654902  , 0.21960784, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01568628, 0.        ,\n",
       "       0.        , 0.21568628, 0.9254902 , 0.89411765, 0.9019608 ,\n",
       "       0.89411765, 0.9411765 , 0.9098039 , 0.8352941 , 0.85490197,\n",
       "       0.8745098 , 0.91764706, 0.8509804 , 0.8509804 , 0.81960785,\n",
       "       0.36078432, 0.        , 0.        , 0.        , 0.00392157,\n",
       "       0.01568628, 0.02352941, 0.02745098, 0.00784314, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.92941177,\n",
       "       0.8862745 , 0.8509804 , 0.8745098 , 0.87058824, 0.85882354,\n",
       "       0.87058824, 0.8666667 , 0.84705883, 0.8745098 , 0.8980392 ,\n",
       "       0.84313726, 0.85490197, 1.        , 0.3019608 , 0.        ,\n",
       "       0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.24313726,\n",
       "       0.5686275 , 0.8       , 0.89411765, 0.8117647 , 0.8352941 ,\n",
       "       0.8666667 , 0.85490197, 0.8156863 , 0.827451  , 0.85490197,\n",
       "       0.8784314 , 0.8745098 , 0.85882354, 0.84313726, 0.8784314 ,\n",
       "       0.95686275, 0.62352943, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.17254902, 0.32156864,\n",
       "       0.41960785, 0.7411765 , 0.89411765, 0.8627451 , 0.87058824,\n",
       "       0.8509804 , 0.8862745 , 0.78431374, 0.8039216 , 0.827451  ,\n",
       "       0.9019608 , 0.8784314 , 0.91764706, 0.6901961 , 0.7372549 ,\n",
       "       0.98039216, 0.972549  , 0.9137255 , 0.93333334, 0.84313726,\n",
       "       0.        , 0.        , 0.22352941, 0.73333335, 0.8156863 ,\n",
       "       0.8784314 , 0.8666667 , 0.8784314 , 0.8156863 , 0.8       ,\n",
       "       0.8392157 , 0.8156863 , 0.81960785, 0.78431374, 0.62352943,\n",
       "       0.9607843 , 0.75686276, 0.80784315, 0.8745098 , 1.        ,\n",
       "       1.        , 0.8666667 , 0.91764706, 0.8666667 , 0.827451  ,\n",
       "       0.8627451 , 0.9098039 , 0.9647059 , 0.        , 0.01176471,\n",
       "       0.7921569 , 0.89411765, 0.8784314 , 0.8666667 , 0.827451  ,\n",
       "       0.827451  , 0.8392157 , 0.8039216 , 0.8039216 , 0.8039216 ,\n",
       "       0.8627451 , 0.9411765 , 0.3137255 , 0.5882353 , 1.        ,\n",
       "       0.8980392 , 0.8666667 , 0.7372549 , 0.6039216 , 0.7490196 ,\n",
       "       0.8235294 , 0.8       , 0.81960785, 0.87058824, 0.89411765,\n",
       "       0.88235295, 0.        , 0.38431373, 0.9137255 , 0.7764706 ,\n",
       "       0.8235294 , 0.87058824, 0.8980392 , 0.8980392 , 0.91764706,\n",
       "       0.9764706 , 0.8627451 , 0.7607843 , 0.84313726, 0.8509804 ,\n",
       "       0.94509804, 0.25490198, 0.28627452, 0.41568628, 0.45882353,\n",
       "       0.65882355, 0.85882354, 0.8666667 , 0.84313726, 0.8509804 ,\n",
       "       0.8745098 , 0.8745098 , 0.8784314 , 0.8980392 , 0.11372549,\n",
       "       0.29411766, 0.8       , 0.83137256, 0.8       , 0.75686276,\n",
       "       0.8039216 , 0.827451  , 0.88235295, 0.84705883, 0.7254902 ,\n",
       "       0.77254903, 0.80784315, 0.7764706 , 0.8352941 , 0.9411765 ,\n",
       "       0.7647059 , 0.8901961 , 0.9607843 , 0.9372549 , 0.8745098 ,\n",
       "       0.85490197, 0.83137256, 0.81960785, 0.87058824, 0.8627451 ,\n",
       "       0.8666667 , 0.9019608 , 0.2627451 , 0.1882353 , 0.79607844,\n",
       "       0.7176471 , 0.7607843 , 0.8352941 , 0.77254903, 0.7254902 ,\n",
       "       0.74509805, 0.7607843 , 0.7529412 , 0.7921569 , 0.8392157 ,\n",
       "       0.85882354, 0.8666667 , 0.8627451 , 0.9254902 , 0.88235295,\n",
       "       0.84705883, 0.78039217, 0.80784315, 0.7294118 , 0.70980394,\n",
       "       0.69411767, 0.6745098 , 0.70980394, 0.8039216 , 0.80784315,\n",
       "       0.4509804 , 0.        , 0.47843137, 0.85882354, 0.75686276,\n",
       "       0.7019608 , 0.67058825, 0.7176471 , 0.76862746, 0.8       ,\n",
       "       0.8235294 , 0.8352941 , 0.8117647 , 0.827451  , 0.8235294 ,\n",
       "       0.78431374, 0.76862746, 0.7607843 , 0.7490196 , 0.7647059 ,\n",
       "       0.7490196 , 0.7764706 , 0.7529412 , 0.6901961 , 0.6117647 ,\n",
       "       0.654902  , 0.69411767, 0.8235294 , 0.36078432, 0.        ,\n",
       "       0.        , 0.2901961 , 0.7411765 , 0.83137256, 0.7490196 ,\n",
       "       0.6862745 , 0.6745098 , 0.6862745 , 0.70980394, 0.7254902 ,\n",
       "       0.7372549 , 0.7411765 , 0.7372549 , 0.75686276, 0.7764706 ,\n",
       "       0.8       , 0.81960785, 0.8235294 , 0.8235294 , 0.827451  ,\n",
       "       0.7372549 , 0.7372549 , 0.7607843 , 0.7529412 , 0.84705883,\n",
       "       0.6666667 , 0.        , 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.25882354, 0.78431374, 0.87058824, 0.92941177,\n",
       "       0.9372549 , 0.9490196 , 0.9647059 , 0.9529412 , 0.95686275,\n",
       "       0.8666667 , 0.8627451 , 0.75686276, 0.7490196 , 0.7019608 ,\n",
       "       0.7137255 , 0.7137255 , 0.70980394, 0.6901961 , 0.6509804 ,\n",
       "       0.65882355, 0.3882353 , 0.22745098, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
       "       0.28235295, 0.16078432, 0.13725491, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Further Preprocessing '''\n",
    "y_train = keras.utils.to_categorical(y_train, 10) # 10 for the number of classes\n",
    "y_test = keras.utils.to_categorical(y_test, 10) # 10 for the number of classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' Making the Model '''\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=784))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3897 - acc: 0.8578 - val_loss: 0.4018 - val_acc: 0.8595\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.3580 - acc: 0.8702 - val_loss: 0.3977 - val_acc: 0.8614\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.3414 - acc: 0.8776 - val_loss: 0.3708 - val_acc: 0.8663\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.3289 - acc: 0.8818 - val_loss: 0.3553 - val_acc: 0.8796\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.3209 - acc: 0.8855 - val_loss: 0.3948 - val_acc: 0.8584\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3125 - acc: 0.8876 - val_loss: 0.3694 - val_acc: 0.8709\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3101 - acc: 0.8889 - val_loss: 0.3597 - val_acc: 0.8829\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3047 - acc: 0.8913 - val_loss: 0.3779 - val_acc: 0.8731\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3006 - acc: 0.8932 - val_loss: 0.4150 - val_acc: 0.8718\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.2966 - acc: 0.8951 - val_loss: 0.3421 - val_acc: 0.8869\n"
     ]
    }
   ],
   "source": [
    "''' Compiling the model '''\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics = ['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                   batch_size = 128,\n",
    "                   epochs = 15,\n",
    "                   verbose=1,\n",
    "                   validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.3420857192158699\n",
      "Test Accuracy:  0.8869\n"
     ]
    }
   ],
   "source": [
    "''' Evaluating the model '''\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
